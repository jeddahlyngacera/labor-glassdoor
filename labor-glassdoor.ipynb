{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABOR – Learning and Building on Reviews\n",
    "\n",
    "**Machine Learning 2.0 Final Project**\n",
    "\n",
    "**MSDS 2020 Learning Team 4**\n",
    "* Ria Ysabelle L. Flora\n",
    "* Crisanto E. Chua\n",
    "* Armand Louis A. De Leon\n",
    "* Jeddahlyn V. Gacera\n",
    "\n",
    "**Asian Institute of Management**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary/Abstract\n",
    "\n",
    "Employee management is one of the most important functions within an organization. Recent studies have shown that employee perceptions of culture, current management, opportunities for growth, and other intangible factors are correlated to a company’s financial well-being. Therefore, it is very important that managers are able to develop necessary skills to effectively connect with employees. It has also given a new dimension to human resource management from being reactionary (i.e. solving employee complaints, etc.) to a more proactive role (giving insights that help in creating policies that prevent or minimize employee dissatisfaction). In this paper, we explore models that predict employee sentiment based on text gathered in employee review data from Glassdoor.com. With the models we have developed, which accurately predict employee sentiment and give insights on what push these ratings, we now are able to provide organizations a new way of better understanding their employees, via internal quarterly reviews or through employee comments in their in-house networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "High employee turnover is one of the biggest challenges faced by businesses today regardless of location, size, nature or business strategy. According to a 2013 study by consumer credit reporting agency, Equifax, 40% of employees who leave their jobs do so within six months of starting a position (Paul, 2013). If this is not addressed organizational cost expenditure will proportionately increase (Ali, 2009).\n",
    "\n",
    "In another study published in 2017, 42% of millennials expect to change jobs every 1 to 3 years, at the very least (Jobvite, 2017). However, what is more disturbing is the fact that in a survey conducted in 2017, only 9% of senior managers believe that turnover is an urgent issue (Pollock, 2017).\n",
    "\n",
    "Online career websites such as Glassdoor.com have provided valuable information on how employees view their current and previous companies and the reasons why they think highly or poorly of them through reviews. The problem however is its qualitative nature which makes it difficult to compare the experience to other companies just based on the text review. A translation of the qualitative review to a quantitative metric can help bridge the gap. Although Glassdoor.com already has the option of adding a quantitative rating (1-5 stars, 5 being the highest), sometimes the rate chosen is not reflective of the actual sentiment of the employee.\n",
    "\n",
    "The goal of the study is to predict quantitative ratings of companies with the use of sentiment analysis on the qualitative text reviews. The model produced could potentially help companies rate themselves accurately with internally generated text reviews or assist other company rating platforms who don’t have quantitative rating schemes transform these into numerical ratings that are based on the same metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "This study is an exploration of the possibilities of creating hybrid NLP models for predictions: using both word embeddings and other numerical features to make a classification prediction. The main reason this is not possible without neural networks (specifically embedding models) is because conventional bag of words creates sparse vector representations of words. This sparse representation makes it such that adding other features to the matrix would have little to no impact.\n",
    "\n",
    "To achieve this, the vector representation of the numerical features is added to the word vector created from word embeddings. The numerical features are probabilities resulting from topic modeling through LDA. A neural network is then trained to make predictions on whether a job review is negative, neutral, or positive.\n",
    "\n",
    "The implementation of our models is based on two different libraries in Python. The topic modeling through LDA was done through the gensim library, which features extensive functionality for more updated NLP methods. Meanwhile, word Embedding and the Stacked GRU were implemented through Tensorflow Keras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:07:58.354335Z",
     "start_time": "2019-12-23T11:07:53.666733Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import string \n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Activation\n",
    "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data gathering\n",
    "\n",
    "This dataset contains company reviews from Glassdoor.com, a jobs and recruiting website that contains a database of millions of company reviews, salary reports, interview reviews, CEO approval ratings, and other information. Before reviews are posted by an employee, they must verify that they currently or previously worked at the listed company. In addition, reviews are completely anonymous and voluntarily contributed by the employee seeking for jobs in other companies in exchange of being able to have unlimited access to the website. These features of Glassdoor ensure the authenticity of the reviews and help reduce reviewer bias.\n",
    "\n",
    "The dataset was obtained by scraping the reviews of 73 Philippine-based companies from Glassdoor.com with 3,718 unique data points. Each of the reviews contains text on the pros and cons of working for the company, advice to management and a short summary of the review. The target variable is the star rating which has a value of 1 to 5 (lowest to highest).\n",
    "\n",
    "<img src=\"img1.PNG\">\n",
    "<p style=\"text-align: center;\">Figure 1 - Sample data from Glassdoor.com</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load target database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:08:01.578175Z",
     "start_time": "2019-12-23T11:08:01.571670Z"
    }
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('glassdoor.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Define functions for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:08:03.279558Z",
     "start_time": "2019-12-23T11:08:03.274224Z"
    }
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/'\n",
    "    '537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:08:03.724975Z",
     "start_time": "2019-12-23T11:08:03.706233Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_from_link(link, soup):\n",
    "    '''Returns a dataframe from extracted data from link using output of \n",
    "       BeautifulSoup.'''\n",
    "    \n",
    "    dict_ = {}\n",
    "    dict_['review_title'] = [i.text.strip('\"') for i in soup.select('h2.h2 span')]\n",
    "    dict_['rating'] = [i['title'] for i in soup.select('span.rating span')][1:]\n",
    "    dict_['job_title'] = [i.text for i in soup.select('span.authorJobTitle')]\n",
    "    dict_['main_text'] = [i.text for i in soup.select('p.mainText')]\n",
    "\n",
    "    reviews = []\n",
    "    for rev in soup.select('div.col-sm-11'):\n",
    "        texts = ''\n",
    "        for j in [i.select('p.strong ~ p') for i in rev]:\n",
    "            if len(j)!=0:\n",
    "                texts += j[0].text+'\\n'\n",
    "        reviews.append(texts)\n",
    "    dict_['review'] = reviews\n",
    "\n",
    "    df = pd.DataFrame.from_dict(dict_)\n",
    "    df['company'] = re.findall(r'(.*?) Reviews ', soup.title.text)[0]\n",
    "    df['link'] = link\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:08:04.207364Z",
     "start_time": "2019-12-23T11:08:04.190513Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_all(link, save_to_db=True):\n",
    "    '''Main scraper function. Returns a dataframe of all data extracted from\n",
    "      all possible pages of given link and appends it to the target db if\n",
    "      save_to_db is True.'''\n",
    "    \n",
    "    df_main = pd.DataFrame(columns=['review_title', 'rating', 'job_title', \n",
    "                                    'main_text', 'review'])\n",
    "\n",
    "    i = 1\n",
    "    while i > 0:\n",
    "        if i==1:\n",
    "            next_link = link\n",
    "        else:\n",
    "            next_link = re.findall(r'(.*).htm', link)[0]+'_P'+str(i)+'.htm'\n",
    "        source = requests.get(next_link, headers=headers)\n",
    "        soup = BeautifulSoup(source.text, 'lxml')\n",
    "        if len(soup.select('h2.h2 span'))==0: \n",
    "            break\n",
    "        df_main = df_main.append(extract_from_link(next_link, soup), sort=False)\n",
    "        print('Extracted', next_link)\n",
    "        i += 1\n",
    "        \n",
    "    df_main = df_main.reset_index(drop=True)\n",
    "    if save_to_db:\n",
    "        df_main.to_sql('reviews_tbl', conn, if_exists='append')\n",
    "        \n",
    "    return df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:08:04.665770Z",
     "start_time": "2019-12-23T11:08:04.656275Z"
    }
   },
   "outputs": [],
   "source": [
    "def pickup_links(links):\n",
    "    '''Returns list of links to scrape by checking if given links already \n",
    "       exist in the target db.'''\n",
    "    try:\n",
    "        extracted_links = pd.read_sql('''SELECT DISTINCT link FROM reviews_tbl''', \n",
    "                                      conn)['link'].values\n",
    "        new_links = [i for i in links if i not in extracted_links]   \n",
    "    except:\n",
    "        new_links = links  \n",
    "    return new_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Provide links to scrape\n",
    "\n",
    "Note that links should follow the format: \n",
    "<pre>\n",
    "https://www.glassdoor.com/Reviews/&lt;company-name-*&gt;.htm\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:08:10.900420Z",
     "start_time": "2019-12-23T11:08:10.894135Z"
    }
   },
   "outputs": [],
   "source": [
    "links = ['https://www.glassdoor.com/Reviews/Edukasyon-ph-Reviews-E1378940.htm',\n",
    "         'https://www.glassdoor.com/Reviews/Asticom-Technologies-Reviews-E1523794.htm',\n",
    "         'https://www.glassdoor.com/Reviews/JeonSoft-Reviews-E1581997.htm',\n",
    "         'https://www.glassdoor.com/Reviews/Vibal-Publishing-House-Reviews-E566841.htm',\n",
    "         'https://www.glassdoor.com/Reviews/AAISI-Reviews-E751354.htm',\n",
    "         'https://www.glassdoor.com/Reviews/Systems-and-Software-Consulting-Group-Reviews-E579828.htm',\n",
    "         'https://www.glassdoor.com/Reviews/Dermclinic-Reviews-E624818.htm',\n",
    "         'https://www.glassdoor.com/Reviews/FilAm-Software-Technology-Reviews-E1017330.htm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Check if links already exist in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:08:13.323493Z",
     "start_time": "2019-12-23T11:08:13.218400Z"
    }
   },
   "outputs": [],
   "source": [
    "new_links = pickup_links(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:47:39.465131Z",
     "start_time": "2019-12-10T16:47:39.457667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.glassdoor.com/Reviews/Edukasyon-ph-Reviews-E1378940.htm',\n",
       " 'https://www.glassdoor.com/Reviews/Asticom-Technologies-Reviews-E1523794.htm',\n",
       " 'https://www.glassdoor.com/Reviews/JeonSoft-Reviews-E1581997.htm',\n",
       " 'https://www.glassdoor.com/Reviews/Vibal-Publishing-House-Reviews-E566841.htm',\n",
       " 'https://www.glassdoor.com/Reviews/AAISI-Reviews-E751354.htm',\n",
       " 'https://www.glassdoor.com/Reviews/Systems-and-Software-Consulting-Group-Reviews-E579828.htm',\n",
       " 'https://www.glassdoor.com/Reviews/Dermclinic-Reviews-E624818.htm',\n",
       " 'https://www.glassdoor.com/Reviews/FilAm-Software-Technology-Reviews-E1017330.htm']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:47:42.648904Z",
     "start_time": "2019-12-10T16:47:42.639658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Scrape data from links and save to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T16:48:20.451286Z",
     "start_time": "2019-12-10T16:47:43.796873Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted https://www.glassdoor.com/Reviews/Edukasyon-ph-Reviews-E1378940.htm\n",
      "Extracted https://www.glassdoor.com/Reviews/Edukasyon-ph-Reviews-E1378940_P2.htm\n",
      "Extracted https://www.glassdoor.com/Reviews/Asticom-Technologies-Reviews-E1523794.htm\n",
      "Extracted https://www.glassdoor.com/Reviews/Asticom-Technologies-Reviews-E1523794_P2.htm\n",
      "Extracted https://www.glassdoor.com/Reviews/JeonSoft-Reviews-E1581997.htm\n",
      "Extracted https://www.glassdoor.com/Reviews/JeonSoft-Reviews-E1581997_P2.htm\n",
      "Extracted https://www.glassdoor.com/Reviews/Vibal-Publishing-House-Reviews-E566841.htm\n",
      "Extracted https://www.glassdoor.com/Reviews/Vibal-Publishing-House-Reviews-E566841_P2.htm\n",
      "Extracted https://www.glassdoor.com/Reviews/AAISI-Reviews-E751354.htm\n",
      "Extracted https://www.glassdoor.com/Reviews/AAISI-Reviews-E751354_P2.htm\n",
      "Extracted https://www.glassdoor.com/Reviews/Systems-and-Software-Consulting-Group-Reviews-E579828.htm\n",
      "Extracted https://www.glassdoor.com/Reviews/Dermclinic-Reviews-E624818.htm\n",
      "Extracted https://www.glassdoor.com/Reviews/FilAm-Software-Technology-Reviews-E1017330.htm\n"
     ]
    }
   ],
   "source": [
    "for link in new_links:\n",
    "    extract_all(link, save_to_db=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Check if data are loaded to the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:08:23.759215Z",
     "start_time": "2019-12-23T11:08:23.708540Z"
    }
   },
   "outputs": [],
   "source": [
    "df_reviews = pd.read_sql('''SELECT * FROM reviews_tbl''', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:08:25.434273Z",
     "start_time": "2019-12-23T11:08:25.426558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3718, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:08:31.121557Z",
     "start_time": "2019-12-23T11:08:31.106339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0    1119\n",
       "3.0     994\n",
       "5.0     839\n",
       "1.0     401\n",
       "2.0     365\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:08:37.692474Z",
     "start_time": "2019-12-23T11:08:37.666021Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>review_title</th>\n",
       "      <th>rating</th>\n",
       "      <th>job_title</th>\n",
       "      <th>main_text</th>\n",
       "      <th>review</th>\n",
       "      <th>company</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3713</td>\n",
       "      <td>5</td>\n",
       "      <td>Doesn't Recommend</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Former Employee - Anonymous Employee</td>\n",
       "      <td>I worked at FilAm Software Technology full-tim...</td>\n",
       "      <td>Free coffee and rice.\\r\\nAnnual increase\\nUsel...</td>\n",
       "      <td>FilAm Software Technology</td>\n",
       "      <td>https://www.glassdoor.com/Reviews/FilAm-Softwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3714</td>\n",
       "      <td>6</td>\n",
       "      <td>'Learn on your own' company</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Developer</td>\n",
       "      <td>I worked at FilAm Software Technology</td>\n",
       "      <td>Good place to start your career as dev in a ha...</td>\n",
       "      <td>FilAm Software Technology</td>\n",
       "      <td>https://www.glassdoor.com/Reviews/FilAm-Softwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3715</td>\n",
       "      <td>7</td>\n",
       "      <td>Working Experience</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Current Employee - Anonymous Employee</td>\n",
       "      <td>I have been working at FilAm Software Technolo...</td>\n",
       "      <td>We rarely have to work over time\\nNo contract ...</td>\n",
       "      <td>FilAm Software Technology</td>\n",
       "      <td>https://www.glassdoor.com/Reviews/FilAm-Softwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3716</td>\n",
       "      <td>8</td>\n",
       "      <td>needs improvement</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Former Employee - Anonymous Employee</td>\n",
       "      <td>I worked at FilAm Software Technology full-time</td>\n",
       "      <td>Annual Increase\\r\\n10 days paid time off for t...</td>\n",
       "      <td>FilAm Software Technology</td>\n",
       "      <td>https://www.glassdoor.com/Reviews/FilAm-Softwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3717</td>\n",
       "      <td>9</td>\n",
       "      <td>A good company</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Applications Developer</td>\n",
       "      <td>I have been working at FilAm Software Technolo...</td>\n",
       "      <td>Agile, Up to date, new hire friendly\\nNone I c...</td>\n",
       "      <td>FilAm Software Technology</td>\n",
       "      <td>https://www.glassdoor.com/Reviews/FilAm-Softwa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                 review_title rating  \\\n",
       "3713      5            Doesn't Recommend    1.0   \n",
       "3714      6  'Learn on your own' company    2.0   \n",
       "3715      7           Working Experience    2.0   \n",
       "3716      8            needs improvement    1.0   \n",
       "3717      9               A good company    5.0   \n",
       "\n",
       "                                  job_title  \\\n",
       "3713   Former Employee - Anonymous Employee   \n",
       "3714                              Developer   \n",
       "3715  Current Employee - Anonymous Employee   \n",
       "3716   Former Employee - Anonymous Employee   \n",
       "3717                 Applications Developer   \n",
       "\n",
       "                                              main_text  \\\n",
       "3713  I worked at FilAm Software Technology full-tim...   \n",
       "3714             I worked at FilAm Software Technology    \n",
       "3715  I have been working at FilAm Software Technolo...   \n",
       "3716    I worked at FilAm Software Technology full-time   \n",
       "3717  I have been working at FilAm Software Technolo...   \n",
       "\n",
       "                                                 review  \\\n",
       "3713  Free coffee and rice.\\r\\nAnnual increase\\nUsel...   \n",
       "3714  Good place to start your career as dev in a ha...   \n",
       "3715  We rarely have to work over time\\nNo contract ...   \n",
       "3716  Annual Increase\\r\\n10 days paid time off for t...   \n",
       "3717  Agile, Up to date, new hire friendly\\nNone I c...   \n",
       "\n",
       "                        company  \\\n",
       "3713  FilAm Software Technology   \n",
       "3714  FilAm Software Technology   \n",
       "3715  FilAm Software Technology   \n",
       "3716  FilAm Software Technology   \n",
       "3717  FilAm Software Technology   \n",
       "\n",
       "                                                   link  \n",
       "3713  https://www.glassdoor.com/Reviews/FilAm-Softwa...  \n",
       "3714  https://www.glassdoor.com/Reviews/FilAm-Softwa...  \n",
       "3715  https://www.glassdoor.com/Reviews/FilAm-Softwa...  \n",
       "3716  https://www.glassdoor.com/Reviews/FilAm-Softwa...  \n",
       "3717  https://www.glassdoor.com/Reviews/FilAm-Softwa...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:08:39.751474Z",
     "start_time": "2019-12-23T11:08:39.723590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   company_count\n",
       "0             73"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql('''SELECT COUNT(DISTINCT company) company_count FROM reviews_tbl''', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preparation and selection\n",
    "\n",
    "Text pre-processing is the first step in preparing text data for analysis. For this study, the following were done to clean the text before part of speech (POS) tagging:\n",
    "1.\tLowercase all characters\n",
    "2.\tRemove all special characters and punctuations\n",
    "3.\tFix word contractions (convert “ain’t” into “is not”)\n",
    "4.\tLemmatizing and POS tagging (done simultaneously)\n",
    "5.\tStop word removal\n",
    "\n",
    "As an additional note, stop words are removed after POS tagging since the model involved in POS tagging may lose information it needs to properly tag words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Load data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:08:42.825136Z",
     "start_time": "2019-12-23T11:08:42.818382Z"
    }
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('glassdoor.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:08:43.484503Z",
     "start_time": "2019-12-23T11:08:43.438338Z"
    }
   },
   "outputs": [],
   "source": [
    "df_reviews = pd.read_sql('''SELECT * FROM reviews_tbl''', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Filter to needed columns: `review` and `rating`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:09:06.692978Z",
     "start_time": "2019-12-23T11:09:06.672861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Very accomodating staff and clean environment\\...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>well known company in the Philippines and in l...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Well we all know that Cebuana is the country's...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>acra acra acra acra acra\\noperations division ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Salary is always on time\\nToo Much Pressure es...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review rating\n",
       "0  Very accomodating staff and clean environment\\...    3.0\n",
       "1  well known company in the Philippines and in l...    5.0\n",
       "2  Well we all know that Cebuana is the country's...    4.0\n",
       "3  acra acra acra acra acra\\noperations division ...    3.0\n",
       "4  Salary is always on time\\nToo Much Pressure es...    3.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_reviews[['review', 'rating']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Define function for text (POS) cleaning and processing\n",
    "\n",
    "**POS Tagging**\n",
    "\n",
    "POS tagging allows a machine to properly identify how a certain word was used within a sentence, whether it was used as a noun, adjective, verb, etc. This has multiple applications, such as in telling a machine how to pronounce a word in text-to-speech (TTS) programs (Bellegarda, 2015), in aspect-level sentiment analysis, or automated grammar checking. POS tagging makes it possible to filter out particular parts of speech that may not be relevant to the analysis, and in a way can be used as a crude means of dimensionality reduction.\n",
    "\n",
    "For this study, the POS tagger in the SpaCy library was used. Hence, lemmatizing is done alongside the POS tagging process. Parts of speech such as nouns, verbs, adjectives, and adverbs were retained for analysis. The study finds that reducing word usage further according to POS caused accuracy to suffer significantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:09:44.443879Z",
     "start_time": "2019-12-23T11:09:44.433742Z"
    }
   },
   "outputs": [],
   "source": [
    "stops = stopwords.words('english') + ['work', 'good']\n",
    "\n",
    "with open('contraction_mapping.json', 'r') as f:\n",
    "    contraction_mapping = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:09:47.187982Z",
     "start_time": "2019-12-23T11:09:47.166530Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text, clean_only=False, \n",
    "               parts_of_speech=['ADJ' ,'NOUN', 'ADV', 'VERB'],\n",
    "              remove_sw=True, sw=stops):\n",
    "    \"\"\"\n",
    "    Cleans text and filters according to part of speech.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "    \n",
    "    clean_only : bool\n",
    "        default at false, will return cleaned string with no tagging\n",
    "    \n",
    "    parts_of_speech : list of strings\n",
    "        refer to parts of speech in SpaCy\n",
    "        \n",
    "    remove_sw : bool\n",
    "    \n",
    "    sw : list of strings\n",
    "        add your own if necessary\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    out3 : str\n",
    "        string with parts of speech filtered\n",
    "    \"\"\"\n",
    "    # cleaning\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    text = re.sub(r'[^\\w\\s]+', ' ', text)\n",
    "    text = re.sub(\"p*\\d\", \"\", text)\n",
    "    text = re.sub(r\" +\", ' ', text)\n",
    "    text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])\n",
    "    \n",
    "    if clean_only == True:\n",
    "        return text  \n",
    "    \n",
    "    else: \n",
    "        # pass text into nlp then remove stopwords\n",
    "        text = nlp(text)\n",
    "\n",
    "        # .lemma_ and .pos_ are helpful extracting the lemmatized\n",
    "        # word and part of speech.\n",
    "\n",
    "        out = []\n",
    "        for token in text:\n",
    "             out.append((token.lemma_, token.pos_))\n",
    "        poss = parts_of_speech\n",
    "\n",
    "        out3 = ''\n",
    "\n",
    "        for item in out:\n",
    "            if item[1] in poss:\n",
    "                out3 = out3 + ' ' + item[0]\n",
    "\n",
    "        if remove_sw:\n",
    "            dummy = out3.split()\n",
    "            dummy = [word for word in dummy if word not in sw]\n",
    "            out3 = ' '.join(dummy)\n",
    "            return out3.strip()\n",
    "\n",
    "        else:\n",
    "            \n",
    "            return out3.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Process `review` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:10:38.967533Z",
     "start_time": "2019-12-23T11:09:51.864008Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "df['review'] = df['review'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:10:38.975515Z",
     "start_time": "2019-12-23T11:10:38.970350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3718, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:10:41.944291Z",
     "start_time": "2019-12-23T11:10:41.926188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>accomodate staff clean environment think con c...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>well know company line money remittance pawn j...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>know country big company pawnshop great benefi...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>acra division family life always render overti...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>salary always time much pressure especially se...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review rating\n",
       "0  accomodate staff clean environment think con c...    3.0\n",
       "1  well know company line money remittance pawn j...    5.0\n",
       "2  know country big company pawnshop great benefi...    4.0\n",
       "3  acra division family life always render overti...    3.0\n",
       "4  salary always time much pressure especially se...    3.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Save cleaned data to a pickle file `DF_glassdoor_3718.pkl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T09:30:22.899755Z",
     "start_time": "2019-12-23T09:30:22.582350Z"
    }
   },
   "source": [
    "```python\n",
    "with open('DF_glassdoor_3718.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Binning the ratings\n",
    "\n",
    "For the particular use case, simplifying the classes from 5 to 3 makes the numbers more interpretable. For example, it is difficult to articulate the difference of a 1 from a 2, and so in. Hence, the combining ratings 1 and 2 into “negative”, 3 into “neutral”, and 4 to 5 into “positive”. This also makes the classification problem simpler for model training.\n",
    "\n",
    "Convert 5-star ratings to 3 classes:\n",
    "\n",
    " * `1`: negative\n",
    " * `2`: neutral\n",
    " * `3`: positive\n",
    "\n",
    "|original rating | new rating |\n",
    "|---|---|\n",
    "|1.0 | 1.0 |\n",
    "|2.0 | 1.0 |\n",
    "|3.0 | 2.0 |\n",
    "|4.0 | 3.0 |\n",
    "|5.0 | 3.0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:10:49.697331Z",
     "start_time": "2019-12-23T11:10:49.685609Z"
    }
   },
   "outputs": [],
   "source": [
    "df['rating'] = df['rating'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:10:50.480729Z",
     "start_time": "2019-12-23T11:10:50.465684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0    1119\n",
       "3.0     994\n",
       "5.0     839\n",
       "1.0     401\n",
       "2.0     365\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:10:51.872989Z",
     "start_time": "2019-12-23T11:10:51.847092Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df['rating']=='2.0', 'rating'] = '1.0'\n",
    "df.loc[df['rating']=='3.0', 'rating'] = '2.0'\n",
    "df.loc[df['rating']=='4.0', 'rating'] = '3.0'\n",
    "df.loc[df['rating']=='5.0', 'rating'] = '3.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:10:53.851459Z",
     "start_time": "2019-12-23T11:10:53.838022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    1958\n",
       "2.0     994\n",
       "1.0     766\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Save 3-class data to a pickle file `df_reviews_3classes.pkl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T00:34:54.881783Z",
     "start_time": "2019-12-23T00:34:54.866602Z"
    }
   },
   "source": [
    "```python\n",
    "with open('df_reviews_3classes.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Topic Modeling using Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Topic modeling is among the common uses of Natural Language Processing (NLP) for extracting main ideas from multiple documents. There are quite a few ways to do this but for purposes of this study, we will use LDA. LDA is a generative probabilistic model that assumes each topic is a combination over a set of words, and each document is a mixture of over a set of topic probabilities.\n",
    "\n",
    "The way LDA does topic modeling is that each topic is simply a collection of words (the topics) in a certain proportion. For example, `topic 0` will be represented as `(Word1*0.007, Word2*0.003, Word3*0.0012, ... Wordn*xxxx)`, where the top most likely words in the topic are displayed alongside their respective probability of occurring. These probabilities of course but sum to `1`. \n",
    "\n",
    "#### Doing LDA in Python\n",
    "\n",
    "For this study, we use the LDA model available through the `gensim` library in Python, which is home to some more recent NLP algorithms. These methods will be discussed later in this section of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Load original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:11:39.373868Z",
     "start_time": "2019-12-23T11:11:39.368749Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle_in = open(\"DF_glassdoor_3718.pkl\",\"rb\")\n",
    "df = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Define function for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_prep(docs):\n",
    "    \"\"\"\n",
    "    Creates a document-term matrix for LDA application.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    docs : list of strings\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    doc_term_matrix : array\n",
    "        Use this as input for the LDA model\n",
    "    dictionary : something\n",
    "        this one too\n",
    "    \"\"\"\n",
    "    docs2 = [x.split(' ') for x in docs]\n",
    "    dictionary = corpora.Dictionary(docs2)\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in docs2]\n",
    "    \n",
    "    return doc_term_matrix, dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Create corpus from reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = list(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_matrix, mdict = lda_prep(corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Creating the object for LDA model using gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Running and training LDA model on the document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = Lda(dt_matrix, num_topics=4, id2word = mdict, passes=50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.028*\"employee\" + 0.012*\"management\" + 0.012*\"benefit\" + 0.011*\"pay\" + 0.010*\"company\" + 0.009*\"salary\" + 0.008*\"people\" + 0.008*\"project\" + 0.007*\"time\" + 0.007*\"lot\"'),\n",
       " (1,\n",
       "  '0.015*\"people\" + 0.015*\"company\" + 0.015*\"employee\" + 0.011*\"time\" + 0.008*\"management\" + 0.007*\"great\" + 0.006*\"hour\" + 0.006*\"salary\" + 0.006*\"go\" + 0.006*\"day\"'),\n",
       " (2,\n",
       "  '0.026*\"company\" + 0.015*\"people\" + 0.014*\"employee\" + 0.009*\"pay\" + 0.009*\"management\" + 0.008*\"benefit\" + 0.008*\"training\" + 0.007*\"salary\" + 0.006*\"great\" + 0.006*\"job\"'),\n",
       " (3,\n",
       "  '0.025*\"salary\" + 0.021*\"employee\" + 0.017*\"benefit\" + 0.013*\"low\" + 0.012*\"company\" + 0.010*\"give\" + 0.009*\"high\" + 0.008*\"people\" + 0.008*\"environment\" + 0.008*\"management\"')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an implementation of pyLDAvis, which visualizes LDA results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "out5 = pyLDAvis.gensim.prepare(ldamodel, dt_matrix, mdict)\n",
    "out5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"out5.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Turning the probabilities into features\n",
    "\n",
    "It is possible to turn the output of the LDA model into features since its assignment of an input string to a topic is probability based: that is, it outputs the probabilities of how likely that string belongs to each topic. In the case of `n=4`, the output per string is an array of length 4, with probabilities per topic. \n",
    "\n",
    "We create the function `get_topic` and use it to produce our features in a new a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic(input_string):\n",
    "    new_str = input_string.split(' ')\n",
    "    new_doc_bow = mdict.doc2bow(new_str)\n",
    "    probs = ldamodel.get_document_topics(new_doc_bow)\n",
    "    probs.sort(key=lambda x: x[0])\n",
    "    l = [x[1] for x in probs]\n",
    "    return l\n",
    "\n",
    "df['topic_cluster'] = df['review'].apply(lambda x: get_topic(x))\n",
    "df_probs = pd.DataFrame(df['topic_cluster'].values.tolist(), columns=['p1','p2','p3','p4']).fillna(0)\n",
    "df_probs['rating']=df['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3713</th>\n",
       "      <td>0.447661</td>\n",
       "      <td>0.540785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3714</th>\n",
       "      <td>0.316928</td>\n",
       "      <td>0.558146</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3715</th>\n",
       "      <td>0.024281</td>\n",
       "      <td>0.506820</td>\n",
       "      <td>0.023928</td>\n",
       "      <td>0.444970</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3716</th>\n",
       "      <td>0.955028</td>\n",
       "      <td>0.014777</td>\n",
       "      <td>0.015083</td>\n",
       "      <td>0.015112</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3717</th>\n",
       "      <td>0.029256</td>\n",
       "      <td>0.029174</td>\n",
       "      <td>0.400387</td>\n",
       "      <td>0.541183</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            p1        p2        p3        p4 rating\n",
       "3713  0.447661  0.540785  0.000000  0.000000    1.0\n",
       "3714  0.316928  0.558146  0.116279  0.000000    2.0\n",
       "3715  0.024281  0.506820  0.023928  0.444970    2.0\n",
       "3716  0.955028  0.014777  0.015083  0.015112    1.0\n",
       "3717  0.029256  0.029174  0.400387  0.541183    5.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_probs.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with open(\"df_probs_4.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_probs, f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deep Learning Classification Model\n",
    "\n",
    "Word embedding is a method in Natural Language Processing (NLP) wherein text data is then transformed into vector representations. With this, the vector representation of each text is then able to capture the semantic and syntactic context of each word relative to the corpus. This simply means that words of similar meanings would be represented with highly similar vectors (Lai, S., et al.,2016). Vectorizing the text data enables its use for further analysis such as machine and statistical analysis with consideration of its context relative to a corpus. \n",
    "\n",
    "In the study, word embedding was used to initialize the text data prior to feeding it into the deep learning classification model. This was executed by incorporating a Tensorflow Embedding layer into the deep learning model pipeline. The Embedding layer serves as a lookup table which maps the words based on its indices to dense vectors, correspondingly its embedding. With this, the model was able to vectorize the text data of each company review through word embedding.\n",
    "\n",
    "<img src=\"img2.PNG\">\n",
    "<p style=\"text-align: center;\">Figure 2. Comparison of LSTM and GRU architecture</p>\n",
    "\n",
    "In classifying the sentiments, as represented by the star ratings in each company review, a stacked Gated Recurrent Unit (GRU) model was used alongside Dense layers and an Activation layer. The GRU layer follows the Recurrent Neural Network (RNN) architecture and is closely comparable to Long Short Term Memory (LSTM) – Figure 1 illustrates the difference between LSTM and GRU models. Moreover, the distinction of GRU models are its reset and update gate. In particular, the model determines how it would integrate previous memory with the new input through the reset gate whereas the update gate determines by how much of the previous memory is to be retained by the model. \n",
    "\n",
    "Given the aforementioned deep learning architecture, a comparison was made between drawing a sentiment analysis by incorporating topic modelling into the deep learning model and directly implementing a deep learning model without topic modelling to execute a sentiment analysis. The first deep learning model merely uses the text data as a corpus without prior classification or clustering whereas the comparative model clusters the corpus into different topics through LDA and uses this clustering to execute a deep learning sentiment analysis per topic cluster.\n",
    "\n",
    "Additionally, in evaluating the model, the model accuracy, precision, and recall, were used in verifying the model’s overall performance in running a sentiment analysis. This value was derived using Tensorflow’s Metric library. The accuracy accounts the ratio between the number of properly classified items relative to the overall count of predicted values. This was further evaluated relative to the Proportion Chance Criterion (PCC) of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Classification Model (without topic modeling)\n",
    "### 4.1. Load 3-class data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:11:49.505575Z",
     "start_time": "2019-12-23T11:11:49.493652Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('df_reviews_3classes.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:11:50.143691Z",
     "start_time": "2019-12-23T11:11:50.134122Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2.0', '3.0', '1.0'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rating.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Balance data by undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:12:36.643501Z",
     "start_time": "2019-12-23T11:12:36.627360Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = df[df.rating=='1.0']\n",
    "df2 = df[df.rating=='2.0']\n",
    "df3 = df[df.rating=='3.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:12:37.061863Z",
     "start_time": "2019-12-23T11:12:37.045660Z"
    }
   },
   "outputs": [],
   "source": [
    "least_count = df.rating.value_counts().min()\n",
    "df1 = df1.sample(least_count)\n",
    "df2 = df2.sample(least_count)\n",
    "df3 = df3.sample(least_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:12:37.580586Z",
     "start_time": "2019-12-23T11:12:37.566957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2298, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df1.append(df2).append(df3).reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Compute baseline (`1.25*PCC`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:12:41.233804Z",
     "start_time": "2019-12-23T11:12:41.226134Z"
    }
   },
   "outputs": [],
   "source": [
    "def pcc(y):\n",
    "    tc = np.unique(y, return_counts=True)[1]\n",
    "    pcc = np.sum((tc/tc.sum())**2)\n",
    "    return pcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:12:42.034061Z",
     "start_time": "2019-12-23T11:12:42.017837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCC = 0.3333333333333333\n",
      "1.25*PCC = 0.41666666666666663\n"
     ]
    }
   ],
   "source": [
    "print('PCC =', pcc(df.rating))\n",
    "print('1.25*PCC =', 1.25*pcc(df.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Convert text to features using `Tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:12:52.819092Z",
     "start_time": "2019-12-23T11:12:52.667834Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=300)\n",
    "tokenizer.fit_on_texts(list(df['review']))\n",
    "X = tokenizer.texts_to_sequences(list(df['review']))\n",
    "X = pad_sequences(X)\n",
    "Y = pd.get_dummies(df['rating']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Split data using `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:12:53.855384Z",
     "start_time": "2019-12-23T11:12:53.840158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: (1608, 357) (1608, 3)\n",
      "test data: (690, 357) (690, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, \n",
    "                                                    random_state = 42)\n",
    "print('train data:', X_train.shape, Y_train.shape)\n",
    "print('test data:', X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Define model callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:12:58.193180Z",
     "start_time": "2019-12-23T11:12:58.185967Z"
    }
   },
   "outputs": [],
   "source": [
    "# checkpoint = ModelCheckpoint(filepath='best_weights.hdf5', save_best_only=True, save_weights_only=True)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. Define NN model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:13:02.666568Z",
     "start_time": "2019-12-23T11:13:00.986173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 357, 100)          30000     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 256)               274944    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 303       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 330,947\n",
      "Trainable params: 330,947\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(300, 100, input_length = X.shape[1]))\n",
    "model.add(GRU(256))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(3, activation='tanh'))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8. Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T07:13:03.186433Z",
     "start_time": "2019-12-23T07:13:03.152283Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for simplicity, only 3 epochs are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T07:13:18.640394Z",
     "start_time": "2019-12-23T07:13:03.188098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1608 samples, validate on 690 samples\n",
      "Epoch 1/3\n",
      "1608/1608 [==============================] - 8s 5ms/sample - loss: 0.6245 - accuracy: 0.6716 - val_loss: 0.5782 - val_accuracy: 0.6995\n",
      "Epoch 2/3\n",
      "1608/1608 [==============================] - 4s 2ms/sample - loss: 0.5639 - accuracy: 0.7102 - val_loss: 0.5652 - val_accuracy: 0.7106\n",
      "Epoch 3/3\n",
      "1608/1608 [==============================] - 4s 2ms/sample - loss: 0.6512 - accuracy: 0.6824 - val_loss: 0.9031 - val_accuracy: 0.5643\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=3, batch_size=32, verbose=1, \n",
    "          callbacks=[checkpoint, lr_reduce], validation_data=(X_test, Y_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9. Load model with best weights\n",
    "\n",
    "After modeling (fitting is incremental so code above was just rerun to improve model), we load the best weight saved to the file defined in the callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:13:22.925168Z",
     "start_time": "2019-12-23T11:13:22.806707Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_weights('best_weights.hdf5')\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10. Predict classification on `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:14:26.161054Z",
     "start_time": "2019-12-23T11:14:19.620338Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "pred_l = np.zeros_like(preds)\n",
    "pred_l[np.arange(len(preds)), preds.argmax(1)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11. Compute accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T10:50:23.661258Z",
     "start_time": "2019-12-23T10:50:23.583905Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.668599\n",
      "Precision: 0.5028986\n",
      "Recall: 0.5028986\n"
     ]
    }
   ],
   "source": [
    "a = Accuracy()\n",
    "a.update_state(Y_test, pred_l)\n",
    "print('Accuracy:', a.result().numpy())\n",
    "\n",
    "p = Precision()\n",
    "p.update_state(Y_test, pred_l)\n",
    "print('Precision:', p.result().numpy())\n",
    "\n",
    "r = Recall()\n",
    "r.update_state(Y_test, pred_l)\n",
    "print('Recall:', r.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deep Learning Classification Model (with topic modeling)\n",
    "### Classification of ratings per LDA cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Load 3-class data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:17:24.133209Z",
     "start_time": "2019-12-23T11:17:24.118113Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('df_reviews_3classes.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:17:24.623574Z",
     "start_time": "2019-12-23T11:17:24.613714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2.0', '3.0', '1.0'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rating.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Load LDA result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:17:25.996814Z",
     "start_time": "2019-12-23T11:17:25.985585Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('df_probs_4.pkl', 'rb') as f:\n",
    "    df2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:17:26.387854Z",
     "start_time": "2019-12-23T11:17:26.365827Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.031846</td>\n",
       "      <td>0.341236</td>\n",
       "      <td>0.347344</td>\n",
       "      <td>0.279573</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.013853</td>\n",
       "      <td>0.548061</td>\n",
       "      <td>0.014329</td>\n",
       "      <td>0.423757</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.377836</td>\n",
       "      <td>0.017038</td>\n",
       "      <td>0.510409</td>\n",
       "      <td>0.094717</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.010039</td>\n",
       "      <td>0.212968</td>\n",
       "      <td>0.139367</td>\n",
       "      <td>0.637626</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.029290</td>\n",
       "      <td>0.441617</td>\n",
       "      <td>0.029088</td>\n",
       "      <td>0.500005</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         p1        p2        p3        p4 rating\n",
       "0  0.031846  0.341236  0.347344  0.279573    3.0\n",
       "1  0.013853  0.548061  0.014329  0.423757    5.0\n",
       "2  0.377836  0.017038  0.510409  0.094717    4.0\n",
       "3  0.010039  0.212968  0.139367  0.637626    3.0\n",
       "4  0.029290  0.441617  0.029088  0.500005    3.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:17:28.626875Z",
     "start_time": "2019-12-23T11:17:28.574057Z"
    }
   },
   "outputs": [],
   "source": [
    "df2['probs'] = df2[['p1', 'p2', 'p3', 'p4']].values.tolist()\n",
    "df2['topic'] = df2['probs'].apply(lambda x: np.argmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Combine the 2 dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:17:29.573736Z",
     "start_time": "2019-12-23T11:17:29.561173Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.join(df2['topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:17:29.933708Z",
     "start_time": "2019-12-23T11:17:29.905463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>accomodate staff clean environment think con c...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>well know company line money remittance pawn j...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>know country big company pawnshop great benefi...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>acra division family life always render overti...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>salary always time much pressure especially se...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review rating  topic\n",
       "0  accomodate staff clean environment think con c...    2.0      2\n",
       "1  well know company line money remittance pawn j...    3.0      1\n",
       "2  know country big company pawnshop great benefi...    3.0      2\n",
       "3  acra division family life always render overti...    2.0      3\n",
       "4  salary always time much pressure especially se...    2.0      3"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Split into clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:17:31.974094Z",
     "start_time": "2019-12-23T11:17:31.952138Z"
    }
   },
   "outputs": [],
   "source": [
    "df_0 = df[df.topic==0]\n",
    "df_1 = df[df.topic==1]\n",
    "df_2 = df[df.topic==2]\n",
    "df_3 = df[df.topic==3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T00:56:28.235808Z",
     "start_time": "2019-12-23T00:56:28.218637Z"
    }
   },
   "source": [
    "### 5.4. A. `Cluster 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:17:32.742047Z",
     "start_time": "2019-12-23T11:17:32.725831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    423\n",
       "1.0    279\n",
       "2.0    213\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_0.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:18:26.618867Z",
     "start_time": "2019-12-23T11:18:26.610508Z"
    }
   },
   "outputs": [],
   "source": [
    "def pcc(y):\n",
    "    tc = np.unique(y, return_counts=True)[1]\n",
    "    pcc = np.sum((tc/tc.sum())**2)\n",
    "    return pcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:18:27.204031Z",
     "start_time": "2019-12-23T11:18:27.185700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCC = 0.3608814834721849\n",
      "1.25*PCC = 0.45110185434023115\n"
     ]
    }
   ],
   "source": [
    "print('PCC =', pcc(df_0.rating))\n",
    "print('1.25*PCC =', 1.25*pcc(df_0.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1. Convert text to features using `Tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:18:30.029338Z",
     "start_time": "2019-12-23T11:18:29.948010Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=300)\n",
    "tokenizer.fit_on_texts(list(df_0['review']))\n",
    "X = tokenizer.texts_to_sequences(list(df_0['review']))\n",
    "X = pad_sequences(X)\n",
    "Y = pd.get_dummies(df_0['rating']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2. Split data using `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:18:30.908228Z",
     "start_time": "2019-12-23T11:18:30.892649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: (640, 353) (640, 3)\n",
      "test data: (275, 353) (275, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, \n",
    "                                                    random_state = 42)\n",
    "print('train data:', X_train.shape, Y_train.shape)\n",
    "print('test data:', X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3. Define model callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:18:36.273708Z",
     "start_time": "2019-12-23T11:18:36.266680Z"
    }
   },
   "outputs": [],
   "source": [
    "# checkpoint = ModelCheckpoint(filepath='best_weights_0.hdf5', save_best_only=True, save_weights_only=True)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.4. Define NN model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:18:38.678368Z",
     "start_time": "2019-12-23T11:18:38.386755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 353, 100)          30000     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 256)               274944    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 303       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 330,947\n",
      "Trainable params: 330,947\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = Sequential()\n",
    "model_0.add(Embedding(300, 100, input_length = X.shape[1]))\n",
    "model_0.add(GRU(256))\n",
    "model_0.add(Dense(100, activation='relu'))\n",
    "model_0.add(Dense(3, activation='tanh'))\n",
    "model_0.add(Activation('softmax'))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.5. Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T01:30:20.067964Z",
     "start_time": "2019-12-23T01:30:19.995915Z"
    }
   },
   "outputs": [],
   "source": [
    "model_0.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for simplicity, only 3 epochs are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T01:30:28.645286Z",
     "start_time": "2019-12-23T01:30:20.069845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 640 samples, validate on 275 samples\n",
      "Epoch 1/3\n",
      "640/640 [==============================] - 5s 8ms/sample - loss: 0.6183 - accuracy: 0.6719 - val_loss: 0.6016 - val_accuracy: 0.6642\n",
      "Epoch 2/3\n",
      "640/640 [==============================] - 2s 2ms/sample - loss: 0.5608 - accuracy: 0.7094 - val_loss: 0.5734 - val_accuracy: 0.7030\n",
      "Epoch 3/3\n",
      "576/640 [==========================>...] - ETA: 0s - loss: 0.5101 - accuracy: 0.7668\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "640/640 [==============================] - 2s 3ms/sample - loss: 0.5030 - accuracy: 0.7724 - val_loss: 0.5537 - val_accuracy: 0.7212\n"
     ]
    }
   ],
   "source": [
    "model_0.fit(X_train, Y_train, epochs=3, batch_size=32, verbose=1, \n",
    "          callbacks=[checkpoint, lr_reduce], validation_data=(X_test, Y_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.6. Load model with best weights\n",
    "\n",
    "After modeling (fitting is incremental so code above was just rerun to improve model), we load the best weight saved to the file defined in the callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:18:49.857784Z",
     "start_time": "2019-12-23T11:18:49.785834Z"
    }
   },
   "outputs": [],
   "source": [
    "model_0.load_weights('best_weights_0.hdf5')\n",
    "model_0.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.7. Predict classification on `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:18:51.943639Z",
     "start_time": "2019-12-23T11:18:51.166616Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = model_0.predict(X_test)\n",
    "pred_l = np.zeros_like(preds)\n",
    "pred_l[np.arange(len(preds)), preds.argmax(1)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.8. Compute accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:18:53.660333Z",
     "start_time": "2019-12-23T11:18:53.618444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7309091\n",
      "Precision: 0.59636366\n",
      "Recall: 0.59636366\n"
     ]
    }
   ],
   "source": [
    "a = Accuracy()\n",
    "a.update_state(Y_test, pred_l)\n",
    "print('Accuracy:', a.result().numpy())\n",
    "\n",
    "p = Precision()\n",
    "p.update_state(Y_test, pred_l)\n",
    "print('Precision:', p.result().numpy())\n",
    "\n",
    "r = Recall()\n",
    "r.update_state(Y_test, pred_l)\n",
    "print('Recall:', r.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T00:56:28.235808Z",
     "start_time": "2019-12-23T00:56:28.218637Z"
    }
   },
   "source": [
    "### 5.4. B. `Cluster 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:19:24.319321Z",
     "start_time": "2019-12-23T11:19:24.305664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    551\n",
       "1.0    246\n",
       "2.0    235\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:19:24.540295Z",
     "start_time": "2019-12-23T11:19:24.532711Z"
    }
   },
   "outputs": [],
   "source": [
    "def pcc(y):\n",
    "    tc = np.unique(y, return_counts=True)[1]\n",
    "    pcc = np.sum((tc/tc.sum())**2)\n",
    "    return pcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:19:24.796233Z",
     "start_time": "2019-12-23T11:19:24.783806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCC = 0.39373948380505974\n",
      "1.25*PCC = 0.49217435475632465\n"
     ]
    }
   ],
   "source": [
    "print('PCC =', pcc(df_1.rating))\n",
    "print('1.25*PCC =', 1.25*pcc(df_1.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.1. Convert text to features using `Tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:19:29.145719Z",
     "start_time": "2019-12-23T11:19:29.058091Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=300)\n",
    "tokenizer.fit_on_texts(list(df_1['review']))\n",
    "X = tokenizer.texts_to_sequences(list(df_1['review']))\n",
    "X = pad_sequences(X)\n",
    "Y = pd.get_dummies(df_1['rating']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2. Split data using `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:19:29.659791Z",
     "start_time": "2019-12-23T11:19:29.645671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: (722, 209) (722, 3)\n",
      "test data: (310, 209) (310, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, \n",
    "                                                    random_state = 42)\n",
    "print('train data:', X_train.shape, Y_train.shape)\n",
    "print('test data:', X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.3. Define model callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:19:33.112712Z",
     "start_time": "2019-12-23T11:19:33.106402Z"
    }
   },
   "outputs": [],
   "source": [
    "# checkpoint = ModelCheckpoint(filepath='best_weights_1.hdf5', save_best_only=True, save_weights_only=True)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.4. Define NN model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:19:34.226574Z",
     "start_time": "2019-12-23T11:19:33.958025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 209, 100)          30000     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 256)               274944    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 303       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 330,947\n",
      "Trainable params: 330,947\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Embedding(300, 100, input_length = X.shape[1]))\n",
    "model_1.add(GRU(256))\n",
    "model_1.add(Dense(100, activation='relu'))\n",
    "model_1.add(Dense(3, activation='tanh'))\n",
    "model_1.add(Activation('softmax'))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.5. Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T01:30:30.272051Z",
     "start_time": "2019-12-23T01:30:30.236268Z"
    }
   },
   "outputs": [],
   "source": [
    "model_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for simplicity, only 3 epochs are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T01:30:37.070273Z",
     "start_time": "2019-12-23T01:30:30.273568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 722 samples, validate on 310 samples\n",
      "Epoch 1/3\n",
      "722/722 [==============================] - 4s 5ms/sample - loss: 0.6001 - accuracy: 0.6814 - val_loss: 0.5904 - val_accuracy: 0.6839\n",
      "Epoch 2/3\n",
      "722/722 [==============================] - 1s 2ms/sample - loss: 0.5607 - accuracy: 0.7322 - val_loss: 0.5559 - val_accuracy: 0.7333\n",
      "Epoch 3/3\n",
      "704/722 [============================>.] - ETA: 0s - loss: 0.4986 - accuracy: 0.7760\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "722/722 [==============================] - 1s 2ms/sample - loss: 0.4984 - accuracy: 0.7761 - val_loss: 0.5641 - val_accuracy: 0.7183\n"
     ]
    }
   ],
   "source": [
    "model_1.fit(X_train, Y_train, epochs=3, batch_size=32, verbose=1, \n",
    "          callbacks=[checkpoint, lr_reduce], validation_data=(X_test, Y_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.6. Load model with best weights\n",
    "\n",
    "After modeling (fitting is incremental so code above was just rerun to improve model), we load the best weight saved to the file defined in the callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:19:38.957496Z",
     "start_time": "2019-12-23T11:19:38.895638Z"
    }
   },
   "outputs": [],
   "source": [
    "model_1.load_weights('best_weights_1.hdf5')\n",
    "model_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.7. Predict classification on `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:19:39.872190Z",
     "start_time": "2019-12-23T11:19:39.337581Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = model_1.predict(X_test)\n",
    "pred_l = np.zeros_like(preds)\n",
    "pred_l[np.arange(len(preds)), preds.argmax(1)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.8. Compute accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:19:41.744396Z",
     "start_time": "2019-12-23T11:19:41.708182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73333335\n",
      "Precision: 0.6\n",
      "Recall: 0.6\n"
     ]
    }
   ],
   "source": [
    "a = Accuracy()\n",
    "a.update_state(Y_test, pred_l)\n",
    "print('Accuracy:', a.result().numpy())\n",
    "\n",
    "p = Precision()\n",
    "p.update_state(Y_test, pred_l)\n",
    "print('Precision:', p.result().numpy())\n",
    "\n",
    "r = Recall()\n",
    "r.update_state(Y_test, pred_l)\n",
    "print('Recall:', r.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T00:56:28.235808Z",
     "start_time": "2019-12-23T00:56:28.218637Z"
    }
   },
   "source": [
    "### 5.4. C. `Cluster 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:19:55.887893Z",
     "start_time": "2019-12-23T11:19:55.874245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    425\n",
       "2.0    214\n",
       "1.0    100\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:19:56.465278Z",
     "start_time": "2019-12-23T11:19:56.457720Z"
    }
   },
   "outputs": [],
   "source": [
    "def pcc(y):\n",
    "    tc = np.unique(y, return_counts=True)[1]\n",
    "    pcc = np.sum((tc/tc.sum())**2)\n",
    "    return pcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:20:01.591779Z",
     "start_time": "2019-12-23T11:20:01.579662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCC = 0.43290955667333797\n",
      "1.25*PCC = 0.5411369458416725\n"
     ]
    }
   ],
   "source": [
    "print('PCC =', pcc(df_2.rating))\n",
    "print('1.25*PCC =', 1.25*pcc(df_2.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.1. Convert text to features using `Tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:20:07.100731Z",
     "start_time": "2019-12-23T11:20:07.038580Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=300)\n",
    "tokenizer.fit_on_texts(list(df_2['review']))\n",
    "X = tokenizer.texts_to_sequences(list(df_2['review']))\n",
    "X = pad_sequences(X)\n",
    "Y = pd.get_dummies(df_2['rating']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.2. Split data using `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:20:07.544272Z",
     "start_time": "2019-12-23T11:20:07.534180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: (517, 80) (517, 3)\n",
      "test data: (222, 80) (222, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, \n",
    "                                                    random_state = 42)\n",
    "print('train data:', X_train.shape, Y_train.shape)\n",
    "print('test data:', X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.3. Define model callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:20:11.080193Z",
     "start_time": "2019-12-23T11:20:11.073851Z"
    }
   },
   "outputs": [],
   "source": [
    "# checkpoint = ModelCheckpoint(filepath='best_weights_2.hdf5', save_best_only=True, save_weights_only=True)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.4. Define NN model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:20:13.300593Z",
     "start_time": "2019-12-23T11:20:13.018113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 80, 100)           30000     \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 256)               274944    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 303       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 330,947\n",
      "Trainable params: 330,947\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(300, 100, input_length = X.shape[1]))\n",
    "model_2.add(GRU(256))\n",
    "model_2.add(Dense(100, activation='relu'))\n",
    "model_2.add(Dense(3, activation='tanh'))\n",
    "model_2.add(Activation('softmax'))\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.5. Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T01:30:38.454414Z",
     "start_time": "2019-12-23T01:30:38.373694Z"
    }
   },
   "outputs": [],
   "source": [
    "model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for simplicity, only 3 epochs are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T01:30:43.460425Z",
     "start_time": "2019-12-23T01:30:38.456728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 517 samples, validate on 222 samples\n",
      "Epoch 1/3\n",
      "517/517 [==============================] - 3s 6ms/sample - loss: 0.5877 - accuracy: 0.6925 - val_loss: 0.5678 - val_accuracy: 0.7177\n",
      "Epoch 2/3\n",
      "517/517 [==============================] - 1s 2ms/sample - loss: 0.5797 - accuracy: 0.7163 - val_loss: 0.5798 - val_accuracy: 0.7177\n",
      "Epoch 3/3\n",
      "517/517 [==============================] - 1s 2ms/sample - loss: 0.5720 - accuracy: 0.7163 - val_loss: 0.5556 - val_accuracy: 0.7177\n"
     ]
    }
   ],
   "source": [
    "model_2.fit(X_train, Y_train, epochs=3, batch_size=32, verbose=1, \n",
    "          callbacks=[checkpoint, lr_reduce], validation_data=(X_test, Y_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.6. Load model with best weights\n",
    "\n",
    "After modeling (fitting is incremental so code above was just rerun to improve model), we load the best weight saved to the file defined in the callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:20:18.655484Z",
     "start_time": "2019-12-23T11:20:18.589046Z"
    }
   },
   "outputs": [],
   "source": [
    "model_2.load_weights('best_weights_2.hdf5')\n",
    "model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.7. Predict classification on `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:20:19.787967Z",
     "start_time": "2019-12-23T11:20:19.138298Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = model_2.predict(X_test)\n",
    "pred_l = np.zeros_like(preds)\n",
    "pred_l[np.arange(len(preds)), preds.argmax(1)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.8. Compute accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:20:20.251871Z",
     "start_time": "2019-12-23T11:20:20.210050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7237237\n",
      "Precision: 0.5855856\n",
      "Recall: 0.5855856\n"
     ]
    }
   ],
   "source": [
    "a = Accuracy()\n",
    "a.update_state(Y_test, pred_l)\n",
    "print('Accuracy:', a.result().numpy())\n",
    "\n",
    "p = Precision()\n",
    "p.update_state(Y_test, pred_l)\n",
    "print('Precision:', p.result().numpy())\n",
    "\n",
    "r = Recall()\n",
    "r.update_state(Y_test, pred_l)\n",
    "print('Recall:', r.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T00:56:28.235808Z",
     "start_time": "2019-12-23T00:56:28.218637Z"
    }
   },
   "source": [
    "### 5.4. D. `Cluster 3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:20:33.452846Z",
     "start_time": "2019-12-23T11:20:33.439522Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    559\n",
       "2.0    332\n",
       "1.0    141\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:20:34.336610Z",
     "start_time": "2019-12-23T11:20:34.325801Z"
    }
   },
   "outputs": [],
   "source": [
    "def pcc(y):\n",
    "    tc = np.unique(y, return_counts=True)[1]\n",
    "    pcc = np.sum((tc/tc.sum())**2)\n",
    "    return pcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:20:38.628654Z",
     "start_time": "2019-12-23T11:20:38.615737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCC = 0.4155643440898984\n",
      "1.25*PCC = 0.519455430112373\n"
     ]
    }
   ],
   "source": [
    "print('PCC =', pcc(df_3.rating))\n",
    "print('1.25*PCC =', 1.25*pcc(df_3.rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.1. Convert text to features using `Tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:20:40.973762Z",
     "start_time": "2019-12-23T11:20:40.905811Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=300)\n",
    "tokenizer.fit_on_texts(list(df_3['review']))\n",
    "X = tokenizer.texts_to_sequences(list(df_3['review']))\n",
    "X = pad_sequences(X)\n",
    "Y = pd.get_dummies(df_3['rating']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.2. Split data using `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:20:41.911480Z",
     "start_time": "2019-12-23T11:20:41.898082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: (722, 140) (722, 3)\n",
      "test data: (310, 140) (310, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, \n",
    "                                                    random_state = 42)\n",
    "print('train data:', X_train.shape, Y_train.shape)\n",
    "print('test data:', X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.3. Define model callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:20:45.465388Z",
     "start_time": "2019-12-23T11:20:45.459046Z"
    }
   },
   "outputs": [],
   "source": [
    "# checkpoint = ModelCheckpoint(filepath='best_weights_3.hdf5', save_best_only=True, save_weights_only=True)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.4. Define NN model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:20:47.751300Z",
     "start_time": "2019-12-23T11:20:47.478476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 140, 100)          30000     \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 256)               274944    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 303       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 330,947\n",
      "Trainable params: 330,947\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3 = Sequential()\n",
    "model_3.add(Embedding(300, 100, input_length = X.shape[1]))\n",
    "model_3.add(GRU(256))\n",
    "model_3.add(Dense(100, activation='relu'))\n",
    "model_3.add(Dense(3, activation='tanh'))\n",
    "model_3.add(Activation('softmax'))\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.5. Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T01:30:44.671197Z",
     "start_time": "2019-12-23T01:30:44.632550Z"
    }
   },
   "outputs": [],
   "source": [
    "model_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for simplicity, only 3 epochs are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T01:30:50.685673Z",
     "start_time": "2019-12-23T01:30:44.673402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 722 samples, validate on 310 samples\n",
      "Epoch 1/3\n",
      "722/722 [==============================] - 4s 5ms/sample - loss: 0.5922 - accuracy: 0.6833 - val_loss: 0.6136 - val_accuracy: 0.6667\n",
      "Epoch 2/3\n",
      "722/722 [==============================] - 1s 2ms/sample - loss: 0.5965 - accuracy: 0.6667 - val_loss: 0.6148 - val_accuracy: 0.6667\n",
      "Epoch 3/3\n",
      "722/722 [==============================] - 1s 2ms/sample - loss: 0.5921 - accuracy: 0.6727 - val_loss: 0.5747 - val_accuracy: 0.7054\n"
     ]
    }
   ],
   "source": [
    "model_3.fit(X_train, Y_train, epochs=3, batch_size=32, verbose=1, \n",
    "          callbacks=[checkpoint, lr_reduce], validation_data=(X_test, Y_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.6. Load model with best weights\n",
    "\n",
    "After modeling (fitting is incremental so code above was just rerun to improve model), we load the best weight saved to the file defined in the callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:20:56.629248Z",
     "start_time": "2019-12-23T11:20:56.566932Z"
    }
   },
   "outputs": [],
   "source": [
    "model_3.load_weights('best_weights_3.hdf5')\n",
    "model_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.7. Predict classification on `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:20:58.020502Z",
     "start_time": "2019-12-23T11:20:57.506652Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = model_3.predict(X_test)\n",
    "pred_l = np.zeros_like(preds)\n",
    "pred_l[np.arange(len(preds)), preds.argmax(1)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.8. Compute accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T11:21:19.096572Z",
     "start_time": "2019-12-23T11:21:19.038411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.70107526\n",
      "Precision: 0.5516129\n",
      "Recall: 0.5516129\n"
     ]
    }
   ],
   "source": [
    "a = Accuracy()\n",
    "a.update_state(Y_test, pred_l)\n",
    "print('Accuracy:', a.result().numpy())\n",
    "\n",
    "p = Precision()\n",
    "p.update_state(Y_test, pred_l)\n",
    "print('Precision:', p.result().numpy())\n",
    "\n",
    "r = Recall()\n",
    "r.update_state(Y_test, pred_l)\n",
    "print('Recall:', r.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Discussion\n",
    "<img src=\"img3.PNG\">\n",
    "<p style=\"text-align: center;\">Figure 3. Topic distribution through LDA on company reviews</p>\n",
    "\n",
    "By running LDA, it has been found that the four distinct clusters are company culture, career growth, renumeration, and management respectively. Furthermore, as seen in Figure 3., it has been found that 35.7% of the company reviews talk about company culture; career growth, renumeration, and management follows with 24.1%, 23.1%, and 17.2% respectively. With this, the results of topic modeling strongly shows that that career growth occurs at 35.7% of the company reviews analyzed and the following topic clusters follow the same interpretation relative to their topic distribution. These topic clusters pertain to the following concerns about the company:\n",
    "* **Topic Cluster 0: Company culture**\n",
    "    - The inter and intra- personal dynamics within the company\n",
    "* **Topic Cluster 1: Career growth**\n",
    "    - Pertains to trainings, accreditations, and recognition in and out of the company\n",
    "* **Topic Cluster 2: Renumeration**\n",
    "    - Pertains to monetary and other benefits given by the company this includes allowances, health insurance, and salary\n",
    "* **Topic Cluster 3: Management**\n",
    "    - Pertains to the overall management on the employees, which encompasses time management, load assignment, and management decisions.\n",
    "\n",
    "<img src=\"img4.PNG\">\n",
    "\n",
    "As seen in the table above, it is apparent that higher accuracies may be derived when a combination of both topic modelling and deep learning models is implemented. Nonetheless, without topic modelling, straightforward GRU model was still able to surpass the benchmark set by the 1.25 PCC by yielding an accuracy of 66.86%. However, taking into account the average accuracy for each of the topic clusters in the hybrid model, it is seen that this has yielded a higher benchmark as given by the 1.25 PCC of 50.10% and accordingly, the models yielded a higher average accuracy of 72.23%. \n",
    "\n",
    "Different baselines were imposed for each topic cluster in the hybrid model due to the imbalance of sentiments within each topic cluster. Nonetheless, by setting the straightforward model without topic modelling as a baseline, shows that each of the topic clusters were able to surpass this set baseline on all accounts of accuracy, precision, and recall. \n",
    "However, a distinct observation may be drawn on the precision and recall draw from each of the models. It is seen that each of the precision and recall for each model have the same values and drawing from the definition of precision and recall, this implies that the false positives and false negatives in the classifications were equal. This further suggests that the models were able to balance out the false negatives and false positives across different classes through its algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion/Recommendation\n",
    "\n",
    "From the results we gathered, we see that utilizing a Stacked GRU model and word embeddings outperforms our baseline model with a 67 % accuracy, with precision and recall scores of both over 50%. Our unique methodology that incorporates topic modeling in our pipeline increased the accuracy up to 73%, with precision and recall scores of 60%, a 10% increase from our original Stacked GRU. This shows that our hybrid model of clustering and deep neural networks is effective in predicting sentiments-based outcomes on textual data.\n",
    "\n",
    "It is worth mentioning that since our model also shows the general reasons that contribute to the sentiment of the review, it is more useful to companies compared to merely providing predictions on the basis of textual review. This added functionality could offer a better understanding of the dynamics affecting employer-employee relations. \n",
    "\n",
    "One of the limitations faced by our study is in the gathering of a larger Philippines-based data set. Compared to other labor markets, the Philippines based companies enrolled in Glassdoor.com is quite small. A more robust dataset would have provided our algorithm with better training data that could improve the prediction performance of our models. Neural networks perform best when trained with larger datasets (Rolnick, 2017). Another limitation is language. The Philippines is a bi-lingual culture, the common vernacular is a mixture of English and Filipino. Some reviews in the website are a mixture of both English and Filipino words. A more inclusive model incorporating Filipino words in the predictions would be ideal.\n",
    "\n",
    "In our study, we have shown that our models could accurately predict the quantitative sentiment of an employee (positive, negative or neutral) from qualitative comments. Although something similar has been done before, i.e. in the prediction of product reviews in Amazon, our approach gives further insights on what reasons are the basis for such a rating. Our Stacked GRU model (without any clustering) could effectively predict sentiment with a 67% accuracy. This can further be improved to up to 73% once clustering of the reasons for giving the rating have been employed. Being able to identify specific topics that contribute to positive or negative ratings could potentially offer managers and other policy-makers in the company deeper insights on how to improve employer-employee relations in the future. Actions that could translate to cost-savings and improve the bottom line of the company. \n",
    "\n",
    "Additional applications of the study include being able to measure employee engagement from internal company feedback mechanisms such as “the voice of the employee” in real time which could provide employers a quick “feel” of the overall morale of the workforce. Further studies could also be done on how to improve accuracies on a 5-category classification model to provide a more detailed prediction of popular rating systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgement\n",
    "\n",
    "Special thanks to Professor Christopher Monterola, Professor Erika Legara, and Associate Professor Eduardo David of the Asian Institute of Management (Manila, Philippines) for their guidance and support. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
